{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Session 2: Topic Modelling\n",
    "\n",
    "In this session, we will focus on a popular kind of probabilistic Machine Learning algorithm. The algorithm itself is called 'latent dirichlet allocation' (LDA), but as it is the most popular algorithm used for 'topic modelling', it is often simply referred to as 'topic modelling'.\n",
    "\n",
    "The aim of topic modelling is quite intuitive. We show the computer a selection of **documents**, and we ask it: What are these documents about? The computer examines the vocabulary of all the documents, and sorts the words into various **topics**. Now a human thinks of a 'topic' as a real-world thing or process which becomes a 'topic' of conversation. We all stand by a forest, point at it, and talk about how beautiful it is. The forest is the 'topic'. A computer cannot do this, so it approaches the question from another angle. It considers a 'topic' to be a set of words that tend to co-occur. When we talk about forests, we would tend to use the words 'tree', 'fox', 'path', 'dark', 'big', 'natural' and so on. We would not tend to use the words 'fricass√©', 'printer' or 'transubstantiation'. When we use a computer to perform topic modelling, it sorts all the words in our corpus into clusters of co-occuring words. These clusters are the **topics**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# The Algorithm\n",
    "\n",
    "The LDA Algorithm can be summarised diagrammatically:\n",
    "\n",
    "![LDA plate notation](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)\n",
    "\n",
    "$M$ denotes the number of documents\n",
    "\n",
    "$N$ is number of words in a given document (document $i$ has $N_{i}$ words)\n",
    "\n",
    "$\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions\n",
    "\n",
    "$\\beta$ is the parameter of the Dirichlet prior on the per-topic word distribution\n",
    "\n",
    "$\\theta_{i}$ is the topic distribution for document $i$\n",
    "\n",
    "$\\phi_{k}$ is the word distribution for topic $k$\n",
    "\n",
    "$z_{ij}$ is the topic for the $j$-th word in document $i$\n",
    "\n",
    "$w_{ij}$ is the specific word.\n",
    "\n",
    "This diagram explains how the model works *generatively*. This is a generative model, because it learns how to create new documents based on the word/topic mixture of the corpus it is trained on. But topic moels are not generally actually *used* to generate text (partly because they have [no concept of word order](https://en.wikipedia.org/wiki/Bag-of-words_model)). Instead, once the topic model is trained, its internal parameters are examined to inform the user about the structure of the corpus, or the model is applied to the text to provide data for further analysis.\n",
    "\n",
    "I step through the diagram in the [slides](slides/topic-modelling.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Topic Modelling in Python\n",
    "\n",
    "Now we have some grasp of what the algorithm is doing, we can learn to apply it in Python using the Gensim package.\n",
    "\n",
    "If you want to do Topic Modelling in R, you can get very similiar results using [very similar code with the help of the MALLET or LDA packages](https://www.tidytextmining.com/topicmodeling.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data\n",
    "\n",
    "For this tutorial we are going to use a small corpus of books from Project Gutenberg, which come included in the Natural Language Toolkit, a very useful text-analysis package for Python. Execute the cell below to import the NLTK and download the Gutenberg books."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "gutenberg = nltk.corpus.gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = gutenberg.fileids()\n",
    "\n",
    "print(f'Downloaded books:')\n",
    "print(\"  - \" + \"\\n  - \".join(books))"
   ]
  },
  {
   "source": [
    "## Preprocesing\n",
    "\n",
    "We will be performing topic modelling using the Gensim library. It requires that the texts be turned into a 'bag of words' model first. A 'bag of words' model is a very simple model of texts, where each row is a *document*, and each column represents a particular *word*. Let's imagine that document 7 is *Moby Dick* and word 2223 is *whale*. In our big bag-of-words table, we would expect the number in row 7, column 2223 to be high, say $2000$. If document 64 is *Pride and Prejudice*, we would expect the number in column 2223 to be $0$, since whales are never mentioned in that novel.\n",
    "\n",
    "If you read the literature in topic modelling, you will see it is common to chunk larger texts such as these into smaller units. However in this case the algorithm works quite well on the entire texts."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary # This will create the 'bag of words'\n",
    "\n",
    "# Initialise the dictionary (this works out the vocab)\n",
    "lower_case_corpus = [[word.lower() for word in gutenberg.words(book)] for book in books]\n",
    "dictionary = Dictionary(lower_case_corpus)\n",
    "\n",
    "# Filter out most and least common words\n",
    "dictionary.filter_n_most_frequent(250)\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.99, keep_n=None)\n",
    "dictionary.compactify()\n",
    "\n",
    "# Create bag-of-words matrix\n",
    "bag_of_words = [dictionary.doc2bow(doc) for doc in lower_case_corpus]"
   ]
  },
  {
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our corpus in the proper format, we can initialise and train a topic model on it. This will produce all the different things described in the diagram above: all the different probability distributions describing which topics are likely to appear where and which words are likely to appear in each topic."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# We need to set some hyperparameters here\n",
    "num_topics = 100\n",
    "chunksize = 9 # There are 18 texts in the corpus\n",
    "passes = 30\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "# Get mapping of word id numbers to the actual words out of the dictionary\n",
    "# id2word = dictionary.id2token <-- this should work but is failing me\n",
    "id2word = {val:key for key,val in dictionary.token2id.items()}\n",
    "\n",
    "# Now initialise and train the model (in Gensim you do this in one step, rather than defining the model then calling the .fit() method)\n",
    "topic_model = LdaModel(\n",
    "    corpus=bag_of_words,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "source": [
    "## Inference\n",
    "\n",
    "Now we have trained the model, we can apply it to a particular text and see how it decomposes the text into topics. Execute the cell below to get the index number for each text in the corpus."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'{\"INDEX\":5s} :: BOOK')\n",
    "for idx, book in enumerate(books):\n",
    "  print(f'{str(idx):5s} :: {book}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_document_topics(bag_of_words[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.show_topic(None, topn=30)"
   ]
  },
  {
   "source": [
    "## Inspecting and Evaluating the Model\n",
    "\n",
    "The model comes with numerous methods we can use to explore its structure. You can see all the methods that come with the model [in the official documentation](https://radimrehurek.com/gensim/models/ldamodel.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the top words for a particular topic\n",
    "# topic_model.show_topic()\n",
    "\n",
    "# To see the top n most significant topics in the corpus\n",
    "# topic_model.show_topics(num_topics=10, num_words=10)\n",
    "\n",
    "# You should be able to calculate the 'log perplexity' using the below code\n",
    "# This is useful if you want to compare the performance of two or more different\n",
    "# models. Otherwise it it quite meaningless.\n",
    "# topic_model.log_perplexity(bag_of_words)"
   ]
  }
 ]
}