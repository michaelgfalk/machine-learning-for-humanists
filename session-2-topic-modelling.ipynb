{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Session 2: Topic Modelling\n",
    "\n",
    "In this session, we will focus on a popular kind of probabilistic Machine Learning algorithm. The algorithm itself is called 'latent dirichlet allocation' (LDA), but as it is the most popular algorithm used for 'topic modelling', it is often simply referred to as 'topic modelling'.\n",
    "\n",
    "The aim of topic modelling is quite intuitive. We show the computer a selection of **documents**, and we ask it: What are these documents about? The computer examines the vocabulary of all the documents, and sorts the words into various **topics**. Now a human thinks of a 'topic' as a real-world thing or process which becomes a 'topic' of conversation. We all stand by a forest, point at it, and talk about how beautiful it is. The forest is the 'topic'. A computer cannot do this, so it approaches the question from another angle. It considers a 'topic' to be a set of words that tend to co-occur. When we talk about forests, we would tend to use the words 'tree', 'fox', 'path', 'dark', 'big', 'natural' and so on. We would not tend to use the words 'fricass√©', 'printer' or 'transubstantiation'. When we use a computer to perform topic modelling, it sorts all the words in our corpus into clusters of co-occuring words. These clusters are the **topics**."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# The Algorithm\n",
    "\n",
    "The LDA Algorithm can be summarised diagrammatically:\n",
    "\n",
    "![LDA plate notation](https://upload.wikimedia.org/wikipedia/commons/4/4d/Smoothed_LDA.png)\n",
    "\n",
    "$M$ denotes the number of documents\n",
    "\n",
    "$N$ is number of words in a given document (document $i$ has $N_{i}$ words)\n",
    "\n",
    "$\\alpha$ is the parameter of the Dirichlet prior on the per-document topic distributions\n",
    "\n",
    "$\\beta$ is the parameter of the Dirichlet prior on the per-topic word distribution\n",
    "\n",
    "$\\theta_{i}$ is the topic distribution for document $i$\n",
    "\n",
    "$\\phi_{k}$ is the word distribution for topic $k$\n",
    "\n",
    "$z_{ij}$ is the topic for the $j$-th word in document $i$\n",
    "\n",
    "$w_{ij}$ is the specific word.\n",
    "\n",
    "This diagram explains how the model works *generatively*. This is a generative model, because it learns how to create new documents based on the word/topic mixture of the corpus it is trained on. But topic moels are not generally actually *used* to generate text (partly because they have [no concept of word order](https://en.wikipedia.org/wiki/Bag-of-words_model)). Instead, once the topic model is trained, its internal parameters are examined to inform the user about the structure of the corpus, or the model is applied to the text to provide data for further analysis.\n",
    "\n",
    "I step through the diagram in the [slides](slides/topic-modelling.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Topic Modelling in Python\n",
    "\n",
    "Now we have some grasp of what the algorithm is doing, we can learn to apply it in Python using the Gensim package.\n",
    "\n",
    "If you want to do Topic Modelling in R, you can get very similiar results using [very similar code with the help of the MALLET or LDA packages](https://www.tidytextmining.com/topicmodeling.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Data\n",
    "\n",
    "For this tutorial we are going to use a small corpus of books from Project Gutenberg, which come included in the Natural Language Toolkit, a very useful text-analysis package for Python. Execute the cell below to import the NLTK and download the Gutenberg books."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Error loading gutenberg: <urlopen error [SSL:\n[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n[nltk_data] Error loading punkt: <urlopen error [SSL:\n[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n[nltk_data]     unable to get local issuer certificate (_ssl.c:1108)>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "gutenberg = nltk.corpus.gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloaded books:\n  - austen-emma.txt\n  - austen-persuasion.txt\n  - austen-sense.txt\n  - bible-kjv.txt\n  - blake-poems.txt\n  - bryant-stories.txt\n  - burgess-busterbrown.txt\n  - carroll-alice.txt\n  - chesterton-ball.txt\n  - chesterton-brown.txt\n  - chesterton-thursday.txt\n  - edgeworth-parents.txt\n  - melville-moby_dick.txt\n  - milton-paradise.txt\n  - shakespeare-caesar.txt\n  - shakespeare-hamlet.txt\n  - shakespeare-macbeth.txt\n  - whitman-leaves.txt\n"
     ]
    }
   ],
   "source": [
    "books = gutenberg.fileids()\n",
    "\n",
    "print(f'Downloaded books:')\n",
    "print(\"  - \" + \"\\n  - \".join(books))"
   ]
  },
  {
   "source": [
    "## Preprocesing\n",
    "\n",
    "We will be performing topic modelling using the Gensim library. It requires that the texts be turned into a 'bag of words' model first. A 'bag of words' model is a very simple model of texts, where each row is a *document*, and each column represents a particular *word*. Let's imagine that document 7 is *Moby Dick* and word 2223 is *whale*. In our big bag-of-words table, we would expect the number in row 7, column 2223 to be high, say $2000$. If document 64 is *Pride and Prejudice*, we would expect the number in column 2223 to be $0$, since whales are never mentioned in that novel.\n",
    "\n",
    "When you are topic modelling larger texts, it can be useful to split them into smaller chunks. The NLTK gutenberg corpus has a useful method that splits all the texts into paragraphs. Then we need to 'tokenise' them (split them into individual words), and then convert them into the 'bag of words model' (the big table saying how many times each word appears in each paragraph)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_sentences = [gutenberg.paras(book) for book in books]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(paragraph_sentences):\n",
    "  \"\"\"Convert list of sentences into list of words\"\"\"\n",
    "  flat_paras = []\n",
    "  for para in paragraph_sentences:\n",
    "    words = [word for sent in para for word in sent]\n",
    "    flat_paras.append(words)\n",
    "  return(flat_paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = [flatten(ps) for ps in paragraph_sentences]\n",
    "\n",
    "docs = {}\n",
    "for book, para_list in zip(books, paragraphs):\n",
    "    n = 0\n",
    "    for paragraph in para_list:\n",
    "        docs[book + \"_\" + n] = paragraph\n",
    "        n += 1\n",
    "\n",
    "total = 0\n",
    "print(\"PARAGRAPHS IN EACH NOVEL:\\n\")\n",
    "for book,para in zip(books, paragraphs):\n",
    "  print(f'{book:25s}  ::  {len(para)}')\n",
    "  total += len(para)\n",
    "print(f\"\\nNUMBER OF DOCUMENTS: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary # This will create the 'bag of words'\n",
    "\n",
    "# Initialise the dictionary (this works out the vocab)\n",
    "dictionary = Dictionary(docs.values())\n",
    "\n",
    "# Filter out the most common words\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=None)\n",
    "\n",
    "# Create bag-of-words matrix\n",
    "bag_of_words = [dictionary.doc2bow(doc) for doc in doc.values()]"
   ]
  },
  {
   "source": [
    "## Training\n",
    "\n",
    "Now that we have our corpus in the proper format, we can initialise and train a topic model on it. This will produce all the different things described in the diagram above: all the different probability distributions describing which topics are likely to appear where and which words are likely to appear in each topic."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "\n",
    "# We need to set some hyperparameters here\n",
    "num_topics = 50\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None\n",
    "\n",
    "# Get mapping of word id numbers to the actual words out of the dictionary\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "# Now initialise and train the model (in Gensim you do this in one step, rather than defining the model then calling the .fit() method)\n",
    "topic_model = LdaModel(\n",
    "    corpus=bag_of_words,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "source": [
    "## Inference\n",
    "\n",
    "Now we have trained the model, we can apply it to a particular paragraph and see how it decomposes the paragraph into topics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_document_topics()\n"
   ]
  },
  {
   "source": [
    "Although we trained the model on the *paragraphs*, we could apply it to a whole book and see how it does.\n",
    "\n",
    "Should we train the model again on just the books?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alice_in_wonderland = dictionary.doc2bow(gutenberg.words(alice_in_wonderland))\n",
    "alice_topics = topic_model.get_document_topics(alice_in_wonderland)"
   ]
  },
  {
   "source": [
    "## Inspecting and Evaluating the Model\n",
    "\n",
    "The model comes with numerous methods we can use to explore its structure. You can see all the methods that come with the model [in the official documentation](https://radimrehurek.com/gensim/models/ldamodel.html)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see the top words for a particular topic\n",
    "# topic_model.show_topic()\n",
    "\n",
    "# To see the top n most significant topics in the corpus\n",
    "# topic_model.show_topics(num_topics=10, num_words=10)\n",
    "\n",
    "# Get the topics with the highest coherence score\n",
    "# topic_model.top_topics()\n",
    "\n",
    "# Calculate the perplexity of the model on a subset of the corpus\n",
    "# (This is meaningless unless you have multiple models to compare)\n",
    "# TEST SAMPLING CODE\n",
    "# topic_model.log_perplexity"
   ]
  }
 ]
}