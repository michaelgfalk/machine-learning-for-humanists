{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49",
   "display_name": "Python 3.8.3 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Session 1: Basics\n",
    "\n",
    "In this Notebook, we will consider some basic concepts of Machine Learning, namely:\n",
    "* Machine Learning vs. Artificial Intelligence\n",
    "* Supervised vs. Unsupervised Methods\n",
    "* Training vs. Inference\n",
    "* Accuracy, Precision and Recall\n",
    "* Some Popular Machine Learning Algorithms\n",
    "\n",
    "You will get the chance to test a little bit of code in this Notebook. In future sessions, we will go into more depth about how to implement particular Machine Learning algorithms.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Machine Learning vs. Artificial Intelligence\n",
    "\n",
    "Machine Learning (ML) and Artificial Intelligence (AI) are closely related, but they aren't the same:\n",
    "* **Machine Learning**: An approach to programming, where the computer learns how to perform a task from data rather than being told what to do.\n",
    "* **Artificial Intelligence**: An aspiration of computer science, to create computers that effectively model, simulate or replicate intelligence.\n",
    "\n",
    "Today, ML and AI are often used synonymously, because most AI researchers use ML methods to try and achieve AI. But this has not always been the case. From the 1940s to the 1980s, the dominant paradigm in AI research was 'symbolic AI'. During this period, AI researchers tried to replicate intelligence by programming computers to manipulate symbols according to pre-programmed logical rules. Only the the 1990s did machine learning become sufficiently successful to take over.\n",
    "\n",
    "The cell below gives a very simple example of how early symbolic AI worked. In a symbolic AI program, you define different *objects* (or *terms* or *arguments*) and *predicates* (or *relations*). The computer can use this set of objects and predicates to describe and reason about the world:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Animal(object):\n",
    "    mortal = True\n",
    "\n",
    "class Human(Animal):\n",
    "    limbs = 4\n",
    "    bipedal = True\n",
    "\n",
    "def is_it(being, prop):\n",
    "    if hasattr(being, prop):\n",
    "        return getattr(being, prop)\n",
    "    else:\n",
    "        print(f'The property {prop} is not defined for this being.')\n",
    "        return None\n",
    "\n",
    "socrates = Human()\n",
    "is_it(socrates, \"mortal\")"
   ]
  },
  {
   "source": [
    "There is still some symbolic AI research going on today. The most famous example is [Cyc](https://cyc.com) (see also the [Wikipedia page](https://en.wikipedia.org/wiki/Cyc)).\n",
    "\n",
    "But even Cyc relies on Machine Learning as well as Symbolic AI. This is because of a recent development in computer science, the rise of:\n",
    "\n",
    "* **Big data**: Big data simply means *lots* of data, and the technology to process it.\n",
    "\n",
    "As we will see in future weeks, computers learn extremely slowly and incrementally. So only if big data is available can ML be really used effectively. [The internet is the main source of big data](https://www.visualcapitalist.com/wp-content/uploads/2019/04/data-generated-each-day-wide.html).\n",
    "\n",
    "Some famous products that rely on big data to enable ML:\n",
    "\n",
    "* [Google Translate](https://translate.google.com/)\n",
    "* OpenAI's [GPT-3 Language Model](https://github.com/openai/gpt-3)\n",
    "* The [Deep Dream Generator](https://deepdreamgenerator.com/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Supervised vs. Unsupervised Learning\n",
    "\n",
    "This is the first big distinction in ML. As the machine learns, it creates a **model** which it uses to analyse input data and give an output. There are two main kinds of model:\n",
    "\n",
    "* **Supervised Learning**: The computer is given some data points, and some correct answers. It creates a model that can predict the given answers based on the provided data.\n",
    "* **Unsupervised Learning**: The computer is given a mass of unstructured data, and tries to find a way to cluster or classify the data without guidance.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training vs. Inference\n",
    "\n",
    "There are many stages to an ML project before the actual machine *learning* can begin: collecting the data, cleaning it, refining the question you want the computer to answer, choosing an algorithm, and setting hyperparameters (i.e choosing settings for the algorithm). Once you have sorted out the general shape of the project, however, a final distinction remains:\n",
    "\n",
    "* **Training**: The learning stage. The computer examines the data and updates its parameters accordingly.\n",
    "* **Inference**: The application stage. The computer examines some fresh data and gives its output or result.\n",
    "\n",
    "In practice, these two steps can be hard to distinguish. Most major AI systems (e.g. Facebook or Google Ads) *infer* things and also *train* themselves constantly. Facebook, for example, constantly predicts which ads and posts you want to see and serves them to you (**inference**), while also observing your responses and updating its models (**training**)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "To give you a practical example of these two distinctions, we are now going to apply two different ML algorithms to the famous 'iris' dataset. This dataset comes preloaded with the `scikit-learn` Python package, which is automatically installed on Google CoCalc (if that is where you are accessing this Notebook)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()\n",
    "print(iris['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f'Four features:\\n{iris[\"feature_names\"]}')\n",
    "print(f'First ten rows:\\n{iris[\"data\"][:10]}')\n",
    "print(f'First ten species: {[iris[\"target_names\"][target] for target in iris[\"target\"][:10]]}')"
   ]
  },
  {
   "source": [
    "The first thing we need to do is chop up the data. We have 150 flowers in the dataset, divided into three main species. We will divide this data into a 'training set' that we will show the algorithm when it is learning, and a 'test set' that we will hide from the algorithm, and then use to see if it has really learnt something from the training data. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(iris[\"data\"], iris[\"target\"], train_size=0.8, stratify=iris[\"target\"])"
   ]
  },
  {
   "source": [
    "First, we will use an **unsupervised** method, called **k-means clustering**. We will tell the computer that the flowers fall into three groups, and see if it can work out what those three groups are."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "# Initialise model\n",
    "kmeans_model = KMeans(n_clusters=3, random_state=1825)\n",
    "# Train the model on the training data (NB: for this unsupervised method, you don't need the 'y' data)\n",
    "kmeans_model.fit(train_x)\n",
    "# Now use the model to cluster the training data:\n",
    "train_x_predictions = kmeans_model.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How closely do these clusters match the actual species?\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# We need to reduce the dimensionality in order to graph it\n",
    "train_x_reduced = PCA(n_components=2).fit_transform(train_x)\n",
    "\n",
    "# Now produce two graphs, one showing the clusters, and the other the actual species\n",
    "plt.figure(1, figsize=[15,6.5])\n",
    "plt.subplot(121)\n",
    "plt.scatter(train_x_reduced[:,0], train_x_reduced[:,1], c=train_x_predictions)\n",
    "plt.title(\"Kmeans clusters (training data)\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(train_x_reduced[:,0], train_x_reduced[:,1], c=train_y)\n",
    "plt.title(\"Actual species (training data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your turn! To reproduce the above graphs, but for the *test* data, simply replace the Nones below with the appropriate code.\n",
    "\n",
    "# First use the model's .predict() method to cluster the test data:\n",
    "test_x_predictions = None\n",
    "\n",
    "# Now use Principal Component Analysis to reduce the number of axes to 2, so we can plot the test data:\n",
    "test_x_reduced = None\n",
    "\n",
    "# Now plot the test data, so we can take a look:\n",
    "plt.figure(1, figsize=[15,6.5])\n",
    "plt.subplot(121)\n",
    "plt.scatter(x=None, y=None, c=None)\n",
    "plt.title(\"Kmeans clusters (test data)\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(x=None, y=None, c=None)\n",
    "plt.title(\"Actual species (test data)\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Another option is to use a **supervised** method. In the next example, we apply the most popular supervised method of all, the Artificial Neural Netowork (also known as a Deep Neural Network, or simply as a Neural Network). In this supervised method, we let the computer know the species of each flower in the test set. It then works backward from this to learn to distinguish different species from one another. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google's TensorFlow is one of the most widely used libraries for neural networks\n",
    "# It can be used with 'keras', a standard interface for Machine Learning,\n",
    "# meaning that the code below looks very similar to the code for kmeans clustering above.\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Initialise the model\n",
    "neural_model = keras.Sequential([\n",
    "    layers.Dense(10),\n",
    "    layers.Dense(5),\n",
    "    layers.Dense(3, activation=\"softmax\") # <-- this must have 3 neurons, as there are three species of flower\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "neural_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"accuracy\") # <-- we need this extra 'compile' step with TensorFlow\n",
    "neural_model.fit(train_x, train_y, epochs=100) # <-- this time, we let it see which species each flower is (`train_y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how it did:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "train_x_neural_probabilities = neural_model.predict(train_x)\n",
    "train_x_neural_predictions = np.argmax(train_x_neural_probabilities, axis=-1)\n",
    "\n",
    "plt.figure(1, figsize=[15,6.5])\n",
    "plt.subplot(121)\n",
    "plt.scatter(train_x_reduced[:,0], train_x_reduced[:,1], c=train_x_neural_predictions)\n",
    "plt.title(\"Predicted species (train data)\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(train_x_reduced[:,0], train_x_reduced[:,1], c=train_y)\n",
    "plt.title(\"Actual species (train data)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn! How did it do on the test data?\n",
    "\n",
    "# Remember, we have already defined the variables `test_x_reduced` and `test_y`, so you can use these again.\n",
    "\n",
    "test_x_neural_probabilities = None\n",
    "test_x_neural_predications = None\n",
    "\n",
    "plt.figure(1, figsize=[15,6.5])\n",
    "plt.subplot(121)\n",
    "plt.scatter(x=None, y=None, c=None)\n",
    "plt.title(\"Predicted species (test data)\")\n",
    "plt.subplot(122)\n",
    "plt.scatter(x=None, y=None, c=None)\n",
    "plt.title(\"Actual species (test data)\")\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "## Accuracy, Precision and Recall\n",
    "\n",
    "We have judged the above two models by their accuracy: How many plants did they categorise correctly? This is only one way of measuring the 'fitness' of a model, however. In this section I introduce two other metrics that are often used for *binary classification problems*. A binary classification problem is one in which there are only two possible answers, e.g. Is this person positive for coronavirus? Can this person speak Twi? Did Shakespeare write this play? When you have such a binary, or yes-no question, then it is common to measure two other things alongside the accuracy:\n",
    "\n",
    "* **Precision**: When the model gives a positive result, how likely is it that it is a true positive? (e.g. This test says a person has COVID. How likely is it that they actually do have COVID?)\n",
    "* **Recall**: If a person really is positive, how likely is it that the model will detect this? (e.g. There are 20 COVID-positive people in this room of 100 people. How many of those 20 will the test detect if we test everyone?)\n",
    "\n",
    "Below is a worked example to show you how in a practical situation, **accuracy**, **precision** and **recall** can be quite disinct from one another. It is always worth thinking about which metrics are most appopriate to your situation.\n",
    "\n",
    "The ten films: *Star Wars*, *Saw*, *Gone With the Wind*, *The Babadook*, *Under the Shadow*, *Citizen Kane*, *Dilwale Dulhaina Le Jayenge*, *Police Story*, *Gol Maal*, *Carrie*.\n",
    "\n",
    "Question: Is this film a horror film?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "actual_answers = np.array([0, 1, 0, 1, 1, 0, 0, 0, 0, 1])\n",
    "predicted_answers = np.array([1, 0, 1, 0, 0, 0, 1, 0, 0, 1])\n",
    "n = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = actual_answers == predicted_answers\n",
    "true_positive = (actual_answers == 1) & (predicted_answers == 1)\n",
    "false_positive = None # When the true answer is negative, but the predicted answer is positive\n",
    "false_negative = None # When the true answer is positive, but the predicted answer is negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = correct.sum() / n\n",
    "precision = None # TP / (TP + FP)\n",
    "recall = None # TP / (FN + FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'The model correctly predicted the genre of {correct.sum()} out of {n} films.')\n",
    "print(f'The model\\'s accuracy is therefore {accuracy}.')\n",
    "print(f'But the model\\'s precision is only {precision}')\n",
    "print(f'And its recall is only {recall:.2f}')"
   ]
  },
  {
   "source": [
    "## Some Popular Kinds of Machine Learning Algorithms\n",
    "\n",
    "* **Bayesian Networks**: Probabalistic models that model how likely a particular outcome is given certain information. These are quite complex but very flexible and easy to interpret. We look at an example of a Bayesian Network in Session 2.\n",
    "* **Deep Learning**: Another synonym for Neural Networks. There are many kinds of Neural Networks for different problems. In Sessions 3 and 4 we will look at two kinds that are particularly interesting to humanists, [Recurrent Neural Networks](https://en.wikipedia.org/wiki/Recurrent_neural_network) for modelling and generating language, and a [Skip-Gram Model](https://en.wikipedia.org/wiki/Word2vec) for trying to mathematically represent the meaning of individual words.\n",
    "* **Genetic Algorithms**: A family of algorithms that mimic natural selection. Different versions of a model are spawned and then compete with one another according to set rules.\n",
    "* **Reinforcement Learning**: A kind of deep learning where an algorithm learns from itself. Such Reinforcement Learning is behind many of the newsworthy AI stories, such as [deepfakes](https://en.wikipedia.org/wiki/Deepfake) or AI grandmasters of [Go](https://www.youtube.com/watch?v=WXuK6gekU1Y) and [Starcraft II](https://www.youtube.com/watch?v=UuhECwm31dM)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Conclusion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "In this week, we have covered some basic Machine Learning concepts, and learnt the basic pattern of machine learning programming in Python. The rough template is:\n",
    "\n",
    "```\n",
    "# Split training and test data:\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y)\n",
    "\n",
    "# Initialise model, and choose settings\n",
    "model = Model(...)\n",
    "\n",
    "# Training: Fit model to training data\n",
    "model.fit(train_x) # <-- unsupervised\n",
    "model.fit(train_x, train_y) # <-- supervised\n",
    "\n",
    "# Inference: Use model to make predictions, categorise inputs etc.\n",
    "predictions = model.predict(test_x)\n",
    "```\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}