{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "colab": {
      "name": "session-3-text-generation.ipynb",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FR1_FT3cHtIu"
      },
      "source": [
        "# Text Generation Using Recurrent Neural Networks\n",
        "\n",
        "This week we are going to learn about Recurrent Neural Networks (RNNs). RNNs are very popular in the field of natural language processing, because they are remarkably good at modelling langauge. This is because, unlike the other models we have looked at so far, RNNs take a account of the *order* in which things appear, and word order is obviously a very important feature of language. Because of their ability to take account of the *order* of observations, RNNs are a type of 'sequence model.'\n",
        "\n",
        "I explained how these models work in the workshop. The [slides](slides/text-generation.pdf) are available in this repo.\n",
        "\n",
        "To see how RNNs work, and to learn a bit abou what they are capable of, today we will train the computer to generate poetry based on 3 million lines of poetry from Project Gutenberg.\n",
        "\n",
        "**NB:** If you are using this notebook in Google Colab, pleaes make sure to click the 'runtime' button in the top right, and choose 'GPU'."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS5YbrZUHtI2"
      },
      "source": [
        "## Import the poetry data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWUsjooGHtI3",
        "outputId": "0507e42f-8326-4ebb-a509-b65635c73e12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download the corpus\n",
        "# !curl -O \"http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz\"\n",
        "\n",
        "# Extract five hundred thousand lines of poetry from the corpus\n",
        "import gzip, json\n",
        "lines = []\n",
        "n = 0\n",
        "for line in gzip.open(\"gutenberg-poetry-v001.ndjson.gz\"):\n",
        "    n += 1\n",
        "    if n > 500000:\n",
        "        break\n",
        "    json_line = json.loads(line.strip())\n",
        "    lines.append(json_line[\"s\"])\n",
        "\n",
        "# Take a look\n",
        "lines[50:60]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70msl4bWHtI4"
      },
      "source": [
        "## Preprocess the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60N25dTRHtI4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "tkzr = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
        "tkzr.fit_on_texts(lines)\n",
        "sequences = tkzr.texts_to_sequences(lines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuujiwqAUM-b"
      },
      "source": [
        "# Add 'newline' to the end of every line of poetry\n",
        "if \"\\n\" in tkzr.word_index:\n",
        "  nl_idx = tkzr.word_index[\"\\n\"]\n",
        "else:\n",
        "  nl_idx = max(tkzr.word_index.values()) + 1\n",
        "  tkzr.word_index[\"\\n\"] = nl_idx\n",
        "  tkzr.index_word[nl_idx] = \"\\n\"\n",
        "for seq in sequences:\n",
        "  seq.append(nl_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wu-2pdjpVz73"
      },
      "source": [
        "# This function reshapes the data into sequences of a fixed length\n",
        "def join_split_sequences(sequences, n=30, pad=0):\n",
        "  \"\"\"Joins sequences and splits them into sequences of width n, with optional\n",
        "  padding of the final split\n",
        "  \n",
        "  Parameters:\n",
        "  ===========\n",
        "  sequences (iterable): an iterable of sequences\n",
        "  n (int): how long the fixed-length sequences should be\n",
        "  pad (int or None): what to pad the end sequence with if desired\n",
        "  \"\"\"\n",
        "  out = []\n",
        "  next_split = []\n",
        "  for seq in sequences:\n",
        "    for token in seq:\n",
        "      next_split.append(token)\n",
        "      if len(next_split) == n:\n",
        "        out.append(next_split)\n",
        "        next_split = []\n",
        "  # Optional padding\n",
        "  if len(out[-1]) < n and pad:\n",
        "    out[-1] += [pad] * (n - len(out))\n",
        "\n",
        "  return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkXf-EiZpSQJ"
      },
      "source": [
        "sequences = join_split_sequences(sequences, n=17, pad=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvwdjfHFrr0I",
        "outputId": "f3e9b0de-0da3-4202-f0e3-c1ca7a515e95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "foo = tkzr.sequences_to_texts([sequences[0]])\n",
        "print(foo[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTF6501qNbUC"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(sequences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RToHXUaayvr1"
      },
      "source": [
        "# From https://www.tensorflow.org/text/tutorials/text_generation\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YDDUnOIy1wd"
      },
      "source": [
        "dataset = dataset.map(split_input_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBqjcglTzDsf",
        "outputId": "54937e56-4c59-4fb6-f97b-422111f7809e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oia2ytPHtI5"
      },
      "source": [
        "## Define the RNN model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HbTCBxcHtI5"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h-Hmp-WzjGC"
      },
      "source": [
        "class PoetryGenerator(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = tf.keras.layers.LSTM(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.lstm.get_initial_state(x)\n",
        "    x, memory_state, carry_state = self.lstm(x, initial_state=states, training=training)\n",
        "    states = [memory_state, carry_state]\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5JpkvfW0PQz"
      },
      "source": [
        "## Initialise and train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIbwRsyr0MEq"
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(tkzr.word_index) + 1\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 32\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 256\n",
        "\n",
        "model = PoetryGenerator(vocab_size, embedding_dim, rnn_units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IAwVBpQ0gAo"
      },
      "source": [
        "model.compile(optimizer='adam', loss=tf.keras.losses.sparse_categorical_crossentropy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3pMu_Nm0zoT",
        "outputId": "3dadc146-b22b-426f-e85f-ef63c5d685a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.fit(dataset, epochs=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfGs4Ph9HtI7"
      },
      "source": [
        "# Generate Some Poetry"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIVuqnCfHtI9"
      },
      "source": [
        "class InferenceModel(tf.keras.Model):\n",
        "  def __init__(self, model, tokenizer, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_ids = [self.tokenizer.word_index[word] for word in inputs]\n",
        "    input_tensor = tf.expand_dims(input_ids, axis=0)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_tensor, states=states, return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    \n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_ids, states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inference_model = InferenceModel(model, tokenizer=tkzr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = (None, None)\n",
        "\n",
        "next_word = ['o', 'for', 'a', 'muse', 'of', 'fire']\n",
        "result = next_word.copy()\n",
        "\n",
        "for n in range(50):\n",
        "    next_word, states = inference_model.generate_one_step(next_word, states)\n",
        "    next_word = next_word.numpy()[0]\n",
        "    next_word = tkzr.index_word[next_word]\n",
        "    result.append(next_word)\n",
        "\n",
        "result = ' '.join(result)\n",
        "\n",
        "print(result)"
      ]
    }
  ]
}